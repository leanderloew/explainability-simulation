{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo of Self Attention in Pytorch \n",
    "\n",
    "In this Notebook we want to test, that Self-Attention and Piece wise Feed Forward Neural Networks can solve a simple logical, deterministic problem. Furthermore, we want to attest to the degree, that the attention weights can \"explain\" the predictions. We see that this is the case for Piece Wise Feed Forward Neural Netowrks but it is not! the case for Self Attention Neural Networks. We conjegture, that this is due to the fact that the representation of Self Attention is dependend on all elements of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from model_pytorch import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,sampler,DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We generate list of range \n",
    "all_data=[]\n",
    "all_label=[]\n",
    "for j in range(100000):\n",
    "    rand_list=[]\n",
    "    for j in range(random.randint(1,100)):\n",
    "        rand_list.append((random.randint(1,20),random.randint(1,20)))\n",
    "    if ((17, 1) in rand_list and (8, 19) in rand_list):\n",
    "        all_label.append(1)            \n",
    "    else:\n",
    "        all_label.append(0)\n",
    "        \n",
    "    all_data.append(np.array(rand_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones=np.array(all_data)[np.where(all_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X,val_X,train_y,val_y = train_test_split(all_data, all_label, test_size=0.1, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dummy_data(Dataset):\n",
    "    def __init__(self, input,output):     \n",
    "        \n",
    "        self.data=input\n",
    "        self.label=output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        X=self.data[index]\n",
    "        y=self.label[index]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds=dummy_data(train_X,train_y)\n",
    "val_ds=dummy_data(val_X,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THe collate difnes how data gets batched \n",
    "def my_collate(batch):\n",
    "    texts=([x[0] for x in batch])\n",
    "    labels=np.array(([x[1] for x in batch]))\n",
    "    batch_size=len(batch)\n",
    "    maxlen=np.max([len(x) for x in texts])\n",
    "    text_stack=np.zeros(shape=(batch_size,maxlen,2))\n",
    "    for enu,txt in enumerate(texts):\n",
    "        text_stack[enu,0:len(txt),:]=txt\n",
    "\n",
    "    return torch.tensor(text_stack[:,:,0]).cuda(),torch.tensor(text_stack[:,:,1]).cuda(),torch.tensor(labels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dl= DataLoader(dataset=val_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=False,\n",
    "                      collate_fn=my_collate\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl= DataLoader(dataset=train_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      collate_fn=my_collate\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class simple_fraud_model(nn.Module):\n",
    "    def __init__(self, d_model=32,heads=1,nlay=1,dropout=0,SelfA=True,return_w=False):\n",
    "        \n",
    "        super(simple_fraud_model,self).__init__()\n",
    "        self.return_w=return_w\n",
    "        emb_d_1=int(d_model/2)\n",
    "        emb_d_2=int(d_model/2)\n",
    "        \n",
    "        self.embedding_1=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_1)\n",
    "        self.embedding_2=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_2)\n",
    "\n",
    "        \n",
    "        if SelfA==True:\n",
    "            self.encoder_layers=EncoderLayer(d_model=d_model,heads=heads,dropout=dropout,share_params=True)\n",
    "\n",
    "        if SelfA==False:\n",
    "            self.encoder_layers=FeedForward(d_model)\n",
    "\n",
    "\n",
    "        self.mula=multi_attention(input_dim=d_model,key_dim=d_model,nheads=1,return_weights=True,value_dim=32)\n",
    "\n",
    "        self.fully_con=nn.Linear(d_model,d_model*4)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.final_fully_con=nn.Linear(d_model*4,1)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.selfa=SelfA\n",
    "        \n",
    "        \n",
    "    def forward(self, x1,x2,return_w):\n",
    "        e1=self.embedding_1(x1)\n",
    "        e2=self.embedding_2(x2)\n",
    "\n",
    "        cat=torch.cat([e1,e2],dim=2)\n",
    "        if self.selfa==True:\n",
    "            \n",
    "            feat,w2=self.encoder_layers(cat)\n",
    "        else:\n",
    "            feat=self.encoder_layers(cat)\n",
    "\n",
    "        ag,weights=self.mula(feat)\n",
    "        fc=self.relu(self.fully_con(ag.squeeze()))\n",
    "        preds=self.sig(self.final_fully_con(fc))\n",
    "        if return_w==False:\n",
    "            return preds.squeeze()\n",
    "        if return_w==True:\n",
    "            if self.selfa==True:\n",
    "                w2=w2.squeeze()\n",
    "                w2=w2.permute((0,2,1))\n",
    "                weights=torch.bmm(w2,weights)\n",
    "                return preds.squeeze(),weights\n",
    "            else:\n",
    "                return preds.squeeze(),weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_eval(atnm,train,opti,crit,eval_metrics,iterator,n_iter,writer):\n",
    "    \n",
    "    if train==False:\n",
    "        loss_val=[]\n",
    "        name1=\"val_loss\"\n",
    "        name2=\"val_roc\"  \n",
    "        atnm.eval()\n",
    "    else:\n",
    "        name1=\"train_loss\"\n",
    "        name2=\"train_roc\"\n",
    "        atnm.train()\n",
    "\n",
    "    if eval_metrics:\n",
    "        store_label=[]\n",
    "        store_preds=[]\n",
    "    #The epcoh \n",
    "    for batch in iterator:\n",
    "        #print(batch[0].shape[1])\n",
    "        opti.zero_grad()\n",
    "        #batch=ba\n",
    "        predictions=atnm(batch[0].long(),batch[1].long(),False)        #print(predictions)\n",
    "        loss = crit(predictions, batch[2].float().cuda())\n",
    "\n",
    "        if train==True:\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            n_iter=n_iter+1\n",
    "            writer.add_scalar(name1,loss.cpu().detach().numpy(),n_iter)\n",
    "        #when we dont train we dont write during epoch but only at the end\n",
    "        #also we dont up the iter\n",
    "        if train==False:\n",
    "            loss_val.append(loss.cpu().detach().numpy())\n",
    "        if eval_metrics== True: \n",
    "            store_preds.append(predictions.cpu().detach().numpy())\n",
    "            store_label.append(batch[2].float().cpu().detach().numpy())\n",
    "            \n",
    "        del predictions\n",
    "        del loss\n",
    "### End of Batch\n",
    "    if train == False:\n",
    "        writer.add_scalar(name1,np.mean(loss_val),n_iter)\n",
    "\n",
    "    if eval_metrics== True: \n",
    "        store_preds=np.concatenate(store_preds)\n",
    "        store_label=np.concatenate(store_label)\n",
    "        roc=roc_auc_score(store_label,store_preds)\n",
    "        writer.add_scalar(name2,roc,n_iter)\n",
    "\n",
    "        return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf=simple_fraud_model(SelfA=True).cuda()\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(sf.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"logs/pff_residual__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8030250539146647\n",
      "0.9672494495399467\n",
      "0.982925807689194\n",
      "0.9997284782563879\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1b60038a2fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                      \u001b[0;34m,\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                    )\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-1dc85194be84>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(atnm, train, opti, crit, eval_metrics, iterator, n_iter, writer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#batch=ba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matnm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#print(predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-a7825b0138d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, return_w)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0me2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfa\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(max_epochs):\n",
    "    #atnm,train,opti,crit,eval_metrics,iterator\n",
    "    n_iter=(train_ds.__len__()/batch_size)*j\n",
    "    roc_t=train_eval(sf,True,optimizer,criterion,True\n",
    "                   ,iter(train_dl),n_iter=n_iter,writer=writer)\n",
    "    roc_v=train_eval(atnm=sf\n",
    "                     ,train=False\n",
    "                     ,opti=optimizer\n",
    "                     ,crit=criterion\n",
    "                     ,eval_metrics=True\n",
    "                     ,iterator=iter(val_dl)\n",
    "                    ,writer=writer\n",
    "                    ,n_iter=n_iter\n",
    "                   )\n",
    "    print(roc_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it=iter(val_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining predictions\n",
    "\n",
    "In this loop we show that the model correctly recovers the \"reason\" for a fraud prediction as being the existence of the two problematic input elements. Here Self Attention generally performs much better. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]\n",
      " [ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n",
      "[[ 1 17]]\n"
     ]
    }
   ],
   "source": [
    "for batch in it:\n",
    "    #We get both inputs\n",
    "    i1=batch[0].long()\n",
    "    i2=batch[1].long()\n",
    "    \n",
    "    #We get both prediction and attention weights \n",
    "    predictions,weights=sf(i1,i2,True)\n",
    "    #we put the predictions and weights to numpy, \n",
    "    preds_np=np.round(predictions.detach().cpu().numpy())\n",
    "    weights_np=weights.detach().cpu().numpy()\n",
    "    #We only select the elments we predicted as a \n",
    "    fraud_elem=np.where(preds_np>0.5)[0]\n",
    "    #A prediction check, that we are correct\n",
    "    if fraud_elem.size>0:\n",
    "        \n",
    "        if not(all(fraud_elem == np.where(batch[2].cpu().numpy())[0])) : \n",
    "            print(\"predicted wrong\")\n",
    "            break\n",
    "            \n",
    "    #if len(np.where(fraud_elem))>0:\n",
    "        i1=i1.detach().cpu().numpy()\n",
    "        i2=i2.detach().cpu().numpy()   \n",
    "        \n",
    "        for fraud_ele in fraud_elem:\n",
    "            my_weight=np.squeeze(np.round(weights_np[fraud_ele],2))\n",
    "            in_elem=np.stack([np.squeeze(i2[fraud_ele]),np.squeeze(i1[fraud_ele])],axis=1)\n",
    "\n",
    "            atn_elem=np.where(my_weight>0.5)[0]\n",
    "            in_elem_j=in_elem[atn_elem]\n",
    "            \n",
    "            print(in_elem_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(17, 1) in rand_list and (8, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_weight[16]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
