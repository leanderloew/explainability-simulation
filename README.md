# selfa-logical-toy
We want to figuere out if self attention can discover simple logical dependencies and explain it correctly. So what we do is we generate sequence of random touples of integers. Whenever a combination of particular touples appears we label it a 1 and if not a zero.

We use a standard network architecture of Embedding, Feature extraction, Aggregation and Fully Connected. As the aggregation mechanism we use Attention. The hope is that this attention mechanism should make this model explainable. We compare two different feature extraction methods: self-attention (selfa) and position-wise-feed-forward (PFF) networks. Note RNNs or CNNs are not fit for this task, because the ordering of the input elements is arbitrary. 

Initially, we only used the sttention weights in the aggregation to make explainability inferences. This lead to generally good perfromance of the PFF model but didnt't work on the self-attention model. However, we could make the selfa model work by incorperating the weights from the feature extraction layer, with a simple matrix multiplication. However, there is now a big issues in terms of finding the best threshold to deem a row significant. For pff the weights where all between 0 and 1. This is now not the case anymore. We got an initial improovement by replacing the softmax normalization of the weights with a modified sigmoid. But finding the correct threshhold is still an open quesiton. 
