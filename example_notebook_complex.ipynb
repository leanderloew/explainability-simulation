{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo of Self Attention in Pytorch \n",
    "\n",
    "In this Notebook we want to test, that Self-Attention and Piece wise Feed Forward Neural Networks can solve a simple logical, deterministic problem. Furthermore, we want to attest to the degree, that the attention weights can \"explain\" the predictions. We see that this is the case for Piece Wise Feed Forward Neural Netowrks but it is not! the case for Self Attention Neural Networks. We conjegture, that this is due to the fact that the representation of Self Attention is dependend on all elements of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from model_pytorch import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,sampler,DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first we get a set of fraud elements (a pair of numbers and a numeric value)\n",
    "n_fraud=20\n",
    "fraud_list=[]\n",
    "\n",
    "for j in range(n_fraud):\n",
    "    fraud_elem=[np.array([random.randint(1,20),random.randint(1,20)]),np.array([random.randint(1,20),random.randint(1,20)]),random.random()]\n",
    "    fraud_list.append(fraud_elem)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([2, 9]), array([ 3, 14]), 0.26655381253138355],\n",
       " [array([ 2, 13]), array([18, 18]), 0.33219769850967984],\n",
       " [array([2, 6]), array([ 5, 11]), 0.5609620089366762],\n",
       " [array([8, 6]), array([ 1, 14]), 0.7735879800290875],\n",
       " [array([20, 13]), array([3, 1]), 0.3154589737358633],\n",
       " [array([15,  4]), array([2, 3]), 0.6671381677849223],\n",
       " [array([5, 1]), array([10, 14]), 0.5734080856809272],\n",
       " [array([ 9, 16]), array([ 2, 10]), 0.3434621581559646],\n",
       " [array([16,  7]), array([20, 17]), 0.5648140371541568],\n",
       " [array([11,  1]), array([13, 17]), 0.43438667569188183],\n",
       " [array([18, 20]), array([16, 17]), 0.6684702219031471],\n",
       " [array([12, 17]), array([2, 6]), 0.7258278416111333],\n",
       " [array([ 3, 16]), array([9, 6]), 0.33490052325216046],\n",
       " [array([18, 13]), array([ 3, 15]), 0.3876556275275983],\n",
       " [array([11,  1]), array([ 7, 19]), 0.08815729492607605],\n",
       " [array([12,  1]), array([12,  8]), 0.40571885207293545],\n",
       " [array([17,  2]), array([10, 16]), 0.9640591723038818],\n",
       " [array([ 8, 18]), array([16,  6]), 0.15376167738406188],\n",
       " [array([18, 15]), array([13, 15]), 0.044129856006443924],\n",
       " [array([ 5, 15]), array([7, 1]), 0.2563623472965425]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the condition: \n",
    "#Both combinations need to be present\n",
    "#and when the first is present on that row the numeric variable needs to be bigger than the value shown here\n",
    "#and on the seconde row it needs to be lower\n",
    "fraud_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then we set up a logical function \n",
    "def add_1(a,b,v,rand_list):\n",
    "    #If i do it this way the ordereing doenst matter !!!! ( have to modify it)\n",
    "    places_a=np.sum(rand_list[:,0:2]==a,axis=1)==2\n",
    "    places_b=np.sum(rand_list[:,0:2]==b,axis=1)==2\n",
    "    #not using the value right now\n",
    "    \n",
    "    if (any(places_a) and any(places_b)): \n",
    "        #and any(rand_list[places_a][:,2]>v) and any(rand_list[places_b][:,2]>v)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we generate the data we will store:\n",
    "\n",
    "#All the input data present\n",
    "all_data=[]\n",
    "#if at least one of the \"reasons\" was present\n",
    "all_label=[]\n",
    "#and the actual reason present ( we can use that to check if the explanation was correct)\n",
    "all_reason=[]\n",
    "\n",
    "for j in range(100000):\n",
    "    rand_list=[]\n",
    "    for j in range(random.randint(1,100)):\n",
    "        rand_list.append((random.randint(1,20),random.randint(1,20),random.random()))\n",
    "    \n",
    "    rand_list=np.array(rand_list)\n",
    "    \n",
    "    add_list=[]\n",
    "    reason_list=[]\n",
    "    \n",
    "    for i in fraud_list:\n",
    "        if add_1(a=i[0],b=i[1],v=i[2],rand_list=rand_list):\n",
    "            \n",
    "            add_list.append(True)\n",
    "            reason_list.append(i)\n",
    "            \n",
    "        else:\n",
    "            add_list.append(False)\n",
    "    if any(add_list):\n",
    "        all_label.append(1) \n",
    "    else:\n",
    "        all_label.append(0)\n",
    "        \n",
    "    all_data.append(rand_list)\n",
    "    all_reason.append(reason_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_share=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split_elem=int((1-test_share)*len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we split into train and validation\n",
    "\n",
    "train_X=all_data[:train_split_elem]\n",
    "val_X=all_data[train_split_elem:]\n",
    "\n",
    "train_y=all_label[:train_split_elem]\n",
    "val_y=all_label[train_split_elem:]\n",
    "\n",
    "train_r=all_reason[:train_split_elem]\n",
    "val_r=all_reason[train_split_elem:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2667888888888889"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2664"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dummy_data(Dataset):\n",
    "    def __init__(self, input,output,reason):     \n",
    "        \n",
    "        self.data=input\n",
    "        self.label=output\n",
    "        self.reason=reason\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        X=self.data[index]\n",
    "        y=self.label[index]\n",
    "        r=self.reason[index]\n",
    "        return X, y, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds=dummy_data(train_X,train_y,train_r)\n",
    "val_ds=dummy_data(val_X,val_y,val_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THe collate difnes how data gets batched \n",
    "#here we are using a simple dynamic badding for batching. \n",
    "def my_collate(batch):\n",
    "    texts=([x[0] for x in batch])\n",
    "    labels=np.array(([x[1] for x in batch]))\n",
    "    reason=np.array(([x[2] for x in batch]))\n",
    "    batch_size=len(batch)\n",
    "    maxlen=np.max([len(x) for x in texts])\n",
    "    text_stack=np.zeros(shape=(batch_size,maxlen,3))\n",
    "    for enu,txt in enumerate(texts):\n",
    "        text_stack[enu,0:len(txt),:]=txt\n",
    "\n",
    "    return torch.tensor(text_stack[:,:,0]).cuda(),torch.tensor(text_stack[:,:,1]).cuda(),torch.tensor(text_stack[:,:,2]).cuda(),torch.tensor(labels).cuda(),reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dl= DataLoader(dataset=val_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=False,\n",
    "                      collate_fn=my_collate\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl= DataLoader(dataset=train_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      collate_fn=my_collate\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class simple_fraud_model(nn.Module):\n",
    "    def __init__(self, d_model=32,heads=1,nlay=1,dropout=0,SelfA=True,return_w=False):\n",
    "        \n",
    "        super(simple_fraud_model,self).__init__()\n",
    "        self.return_w=return_w\n",
    "        emb_d_1=int(d_model/2)\n",
    "        emb_d_2=int(d_model/2)\n",
    "        \n",
    "        self.embedding_1=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_1)\n",
    "        self.embedding_2=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_2-1)\n",
    "\n",
    "        \n",
    "        if SelfA==True:\n",
    "            self.encoder_layers=EncoderLayer(d_model=d_model,heads=heads,dropout=dropout,share_params=True)\n",
    "\n",
    "        if SelfA==False:\n",
    "            self.encoder_layers=FeedForward(d_model)\n",
    "\n",
    "\n",
    "        self.mula=multi_attention(input_dim=d_model,key_dim=d_model,nheads=1,return_weights=True,value_dim=d_model)\n",
    "\n",
    "        self.fully_con=nn.Linear(d_model,d_model*4)\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        self.fully_con_1=nn.Linear(d_model*4,d_model*4)\n",
    "        self.relu_1=nn.ReLU()\n",
    "        \n",
    "        self.final_fully_con=nn.Linear(d_model*4,1)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.selfa=SelfA\n",
    "        \n",
    "        \n",
    "    def forward(self, x1,x2,x3,return_w):\n",
    "        e1=self.embedding_1(x1)\n",
    "        e2=self.embedding_2(x2)\n",
    "\n",
    "        cat=torch.cat([e1,e2,x3.unsqueeze(2)],dim=2)\n",
    "        if self.selfa==True:\n",
    "            \n",
    "            feat,w2=self.encoder_layers(cat)\n",
    "        else:\n",
    "            feat=self.encoder_layers(cat)\n",
    "\n",
    "        ag,weights=self.mula(feat)\n",
    "        #first layer with a relu activation\n",
    "        fc=self.relu(self.fully_con(ag.squeeze()))\n",
    "        #second layer\n",
    "        fc=self.relu_1(self.fully_con_1(fc))\n",
    "        #finally the last layer with a sigmoid\n",
    "        preds=self.sig(self.final_fully_con(fc))\n",
    "        \n",
    "        if return_w==False:\n",
    "            return preds.squeeze()\n",
    "        if return_w==True:\n",
    "            \n",
    "            if self.selfa==True:\n",
    "                #this might not be a 100% correct\n",
    "                #We are multiplying the first layer attention weights together with the aggregation weights. \n",
    "                return preds.squeeze(),weights#torch.bmm(w2.squeeze(),weights)\n",
    "            else:\n",
    "                return preds.squeeze(),weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A train and evaluation funciton\n",
    "def train_eval(atnm,train,opti,crit,eval_metrics,iterator,n_iter,writer):\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "    atnm: A Model to be trained/evalued\n",
    "    train: If we want to train/eval (If train we sub the gradient)\n",
    "    opti: An optimizer to be used\n",
    "    crit: A loss function to be used\n",
    "    eval_matrics: If we want to keep track of predictions during batch gen and in the end\n",
    "    calculate a metric on the whole data (aka the AUC)\n",
    "    iterator: The data generator as an iterator\n",
    "    n_iter: the current step to be updated\n",
    "    writer: the tensorboard writer used to keep trakc of training results\n",
    "    \n",
    "    '''\n",
    "    if train==False:\n",
    "        loss_val=[]\n",
    "        name1=\"val_loss\"\n",
    "        name2=\"val_roc\"  \n",
    "        atnm.eval()\n",
    "    else:\n",
    "        name1=\"train_loss\"\n",
    "        name2=\"train_roc\"\n",
    "        atnm.train()\n",
    "\n",
    "    if eval_metrics:\n",
    "        store_label=[]\n",
    "        store_preds=[]\n",
    "        \n",
    "    #The epcoh \n",
    "    for batch in iterator:\n",
    "        #print(batch[0].shape[1])\n",
    "        opti.zero_grad()\n",
    "        #batch=ba\n",
    "        predictions,w=atnm(batch[0].long(),batch[1].long(),batch[2].float(),True)        #print(predictions)\n",
    "        loss = crit(predictions, batch[3].float().cuda())\n",
    "\n",
    "        if train==True:\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            n_iter=n_iter+1\n",
    "            writer.add_scalar(name1,loss.cpu().detach().numpy(),n_iter)\n",
    "        #when we dont train we dont write during epoch but only at the end\n",
    "        #also we dont up the iter\n",
    "        if train==False:\n",
    "            loss_val.append(loss.cpu().detach().numpy())\n",
    "        if eval_metrics== True: \n",
    "            store_preds.append(predictions.cpu().detach().numpy())\n",
    "            store_label.append(batch[3].float().cpu().detach().numpy())\n",
    "            \n",
    "        del predictions\n",
    "        del loss\n",
    "        \n",
    "    ### End of Batch\n",
    "    if train == False:\n",
    "        writer.add_scalar(name1,np.mean(loss_val),n_iter)\n",
    "\n",
    "    if eval_metrics== True: \n",
    "        store_preds=np.concatenate(store_preds)\n",
    "        store_label=np.concatenate(store_label)\n",
    "        roc=roc_auc_score(store_label,store_preds)\n",
    "        writer.add_scalar(name2,roc,n_iter)\n",
    "        \n",
    "        return roc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining predictions\n",
    "\n",
    "In this loop we show that the model correctly recovers the \"reason\" for a fraud prediction as being the existence of the two problematic input elements. Here Self Attention generally performs much better. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explanation_score(sf,it):\n",
    "    reason_list=[]\n",
    "    explanation_list=[]\n",
    "    weight_list=[]\n",
    "    sf.eval()\n",
    "    \n",
    "    for batch in it:\n",
    "\n",
    "        #We get both inputs\n",
    "        i1=batch[0].long()\n",
    "        i2=batch[1].long()\n",
    "        i3=batch[2].float()\n",
    "\n",
    "        #this is directly a list \n",
    "        r=batch[4]\n",
    "\n",
    "        #We get both prediction and attention weights \n",
    "        predictions,weights=sf(i1,i2,batch[2].float(),True)\n",
    "\n",
    "        #Get the predictions\n",
    "        preds_np=np.round(predictions.detach().cpu().numpy())\n",
    "\n",
    "        #get the attention weights \n",
    "        weights_np=weights.detach().cpu().numpy()\n",
    "        fraud_elem=np.where(preds_np>0.5)[0]\n",
    "\n",
    "        #Only if we actually predict something we care about an explanation (we can change that threshhold)\n",
    "        if fraud_elem.size>0:\n",
    "            \n",
    "            i1=i1.detach().cpu().numpy()\n",
    "            i2=i2.detach().cpu().numpy()\n",
    "            i3=i3.detach().cpu().numpy()\n",
    "\n",
    "            #This is the reason\n",
    "            for fraud_ele in fraud_elem:\n",
    "\n",
    "                #get the weigths(we round casue we want to look at them later)\n",
    "                my_weight=np.squeeze(np.round(weights_np[fraud_ele],2))\n",
    "\n",
    "                #get the element in the input the model put attention on\n",
    "                atn_elem=np.where(my_weight>0.5)[0]\n",
    "\n",
    "                #get the actual value so we can check if the actual confidence is a good measure\n",
    "                atn_value=np.expand_dims(my_weight[np.where(my_weight>0.5)[0]],axis=1)\n",
    "\n",
    "                #Next we stack up all inputs \n",
    "                in_elem=np.stack([np.squeeze(i1[fraud_ele]),np.squeeze(i2[fraud_ele]),np.squeeze(i3[fraud_ele])],axis=1)\n",
    "\n",
    "                #we get the input elements our model has picked out \n",
    "                in_elem_j=in_elem[atn_elem]\n",
    "            \n",
    "                #then we concatenate together with the other variables\n",
    "                in_elem_j=np.concatenate([in_elem_j,atn_value],axis=1)\n",
    "\n",
    "                #in the end we save everything in a list for evaluation\n",
    "                explanation_list.append(in_elem_j)\n",
    "                reason_list.append(r[fraud_ele])\n",
    "                \n",
    "                \n",
    "    if (len(reason_list))>0 and (len(explanation_list))>0:\n",
    "        error_list=[]\n",
    "        reason_wrong=[]\n",
    "\n",
    "        for explanation,reason in zip(explanation_list,reason_list):\n",
    "            e=explanation\n",
    "            r=reason\n",
    "            if len(e)>0:\n",
    "                splits=np.split(e[:,0:4],axis=0,indices_or_sections=len(e))\n",
    "\n",
    "                if len(r)>0:\n",
    "                    e=np.array([j[0][0:2] for j in splits])\n",
    "                    #new_array = [tuple(row) for row in e]\n",
    "                    #e = np.unique(new_array)\n",
    "\n",
    "                    e=np.unique(e,axis=0)\n",
    "\n",
    "                    r=np.array(r[0][0:2])\n",
    "\n",
    "                    #if not np.array_equal(np.sort(r.flatten()),np.sort(e.flatten())):\n",
    "                    error_list.append(splits)\n",
    "                    reason_wrong.append(r)\n",
    "                else:\n",
    "                    error_list.append(splits)\n",
    "                    reason_wrong.append(r)\n",
    "            else:\n",
    "                error_list.append(e)\n",
    "                reason_wrong.append(r)        \n",
    "                    #print(reason_wrong)\n",
    "        #print(reason_list)\n",
    "        #so we define it as an error if we did not return the exact two conditions that lead to a \"fraud\" in a particular claim \n",
    "        error_score=len(error_list)/len(reason_list)\n",
    "        \n",
    "        return error_score,error_list,reason_wrong\n",
    "    else:\n",
    "        return 1,[],[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf=simple_fraud_model(SelfA=False,d_model=32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion=nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(sf.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"logs/pff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epochs=200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7946640410857968\n",
      "1.0\n",
      "0.5461465271170314\n",
      "53.97621313035204\n",
      "0.7967356157957304\n",
      "1.0\n",
      "0.6482558139534884\n",
      "61.87063953488372\n",
      "0.799437924497562\n",
      "1.0\n",
      "0.8029045643153527\n",
      "67.03941908713693\n",
      "0.8025201626108115\n",
      "1.0\n",
      "0.7929901423877328\n",
      "64.08652792990142\n",
      "0.8054224651314347\n",
      "1.0\n",
      "0.8195329087048833\n",
      "58.25053078556263\n",
      "0.8083457213347479\n",
      "1.0\n",
      "0.762970498474059\n",
      "49.38250254323499\n",
      "0.8127887719371498\n",
      "1.0\n",
      "0.7372188139059305\n",
      "40.53169734151329\n",
      "0.8175473558345696\n",
      "1.0\n",
      "0.6185682326621924\n",
      "32.115212527964204\n",
      "0.8224563764282276\n",
      "1.0\n",
      "0.584703947368421\n",
      "23.06578947368421\n",
      "0.8287392320073619\n",
      "1.0\n",
      "0.5867346938775511\n",
      "17.008163265306123\n",
      "0.8341078520587109\n",
      "1.0\n",
      "0.5909090909090909\n",
      "12.984848484848484\n",
      "0.8390099392604163\n",
      "1.0\n",
      "0.640807651434644\n",
      "12.075451647183847\n",
      "0.8430949351750878\n",
      "1.0\n",
      "0.6409978308026031\n",
      "10.582429501084599\n",
      "0.8459613426812853\n",
      "1.0\n",
      "0.6410496719775071\n",
      "9.082474226804123\n",
      "0.8494498622122668\n",
      "1.0\n",
      "0.6483300589390962\n",
      "9.003929273084479\n",
      "0.8515582273931511\n",
      "1.0\n",
      "0.6838021338506305\n",
      "8.881668283220174\n",
      "0.8530185378944922\n",
      "1.0\n",
      "0.7001862197392924\n",
      "8.756983240223464\n",
      "0.8550903940336192\n",
      "1.0\n",
      "0.7158634538152611\n",
      "8.488955823293173\n",
      "0.8562882334351799\n",
      "1.0\n",
      "0.7276669557675629\n",
      "8.439722463139637\n",
      "0.8570962678190731\n",
      "1.0\n",
      "0.7430497051390059\n",
      "8.721988205560235\n",
      "0.8586819166494737\n",
      "1.0\n",
      "0.7465437788018433\n",
      "8.841013824884792\n",
      "0.8587358487167648\n",
      "1.0\n",
      "0.7691082802547771\n",
      "8.847929936305732\n",
      "0.8600467203162814\n",
      "1.0\n",
      "0.7703389830508475\n",
      "8.794915254237289\n",
      "0.8613029690677593\n",
      "1.0\n",
      "0.7759562841530054\n",
      "8.979052823315119\n",
      "0.8623311834189697\n",
      "1.0\n",
      "0.7782608695652173\n",
      "8.922608695652174\n",
      "0.8636205640618808\n",
      "1.0\n",
      "0.8294701986754967\n",
      "9.187086092715232\n",
      "0.8650314965319736\n",
      "1.0\n",
      "0.8254980079681274\n",
      "9.138645418326693\n",
      "0.8649994391883704\n",
      "1.0\n",
      "0.8334841628959276\n",
      "9.409049773755656\n",
      "0.8660556685365847\n",
      "1.0\n",
      "0.8295019157088123\n",
      "9.42432950191571\n",
      "0.8662872591784805\n",
      "1.0\n",
      "0.8319604612850082\n",
      "9.200988467874794\n",
      "0.8661431674313355\n",
      "1.0\n",
      "0.8333333333333334\n",
      "9.352136752136753\n",
      "0.8671540099259565\n",
      "1.0\n",
      "0.8314332247557004\n",
      "9.180781758957655\n",
      "0.8675317902417139\n",
      "1.0\n",
      "0.8316412859560067\n",
      "9.21404399323181\n",
      "0.8673111497538979\n",
      "1.0\n",
      "0.8280757097791798\n",
      "9.091482649842272\n",
      "0.8688275414181901\n",
      "1.0\n",
      "0.8271417740712661\n",
      "9.082638362395754\n",
      "0.8692519366422038\n",
      "1.0\n",
      "0.8240997229916898\n",
      "8.828947368421053\n",
      "0.870059101154044\n",
      "1.0\n",
      "0.8363780778395552\n",
      "8.806989674344718\n",
      "0.8704686829686831\n",
      "1.0\n",
      "0.8378812199036918\n",
      "8.792134831460674\n",
      "0.8705288576471784\n",
      "1.0\n",
      "0.83671875\n",
      "8.79921875\n",
      "0.8716155836861944\n",
      "1.0\n",
      "0.8333333333333334\n",
      "8.53062678062678\n",
      "0.8718101791813625\n",
      "1.0\n",
      "0.8419405320813772\n",
      "8.687793427230046\n",
      "0.8730022109077454\n",
      "1.0\n",
      "0.8345272206303725\n",
      "8.535100286532952\n",
      "0.8739154998100609\n",
      "1.0\n",
      "0.832\n",
      "8.55709090909091\n",
      "0.8753812342194977\n",
      "1.0\n",
      "0.8427719821162444\n",
      "8.60059612518629\n",
      "0.8762965954640574\n",
      "1.0\n",
      "0.8426356589147287\n",
      "8.651937984496124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b71fd80807ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     roc_t=train_eval(sf,True,optimizer,criterion,True\n\u001b[0;32m---> 10\u001b[0;31m                    ,iter(train_dl),n_iter=n_iter,writer=writer)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     roc_v=train_eval(atnm=sf\n",
      "\u001b[0;32m<ipython-input-19-01964237f461>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(atnm, train, opti, crit, eval_metrics, iterator, n_iter, writer)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m#when we dont train we dont write during epoch but only at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#also we dont up the iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sensitivity_list=[]\n",
    "totally_correct_list=[]\n",
    "auc_list=[]\n",
    "\n",
    "for j in range(max_epochs):\n",
    "    #atnm,train,opti,crit,eval_metrics,iterator\n",
    "    n_iter=(train_ds.__len__()/batch_size)*j\n",
    "    \n",
    "    roc_t=train_eval(sf,True,optimizer,criterion,True\n",
    "                   ,iter(train_dl),n_iter=n_iter,writer=writer)\n",
    "    \n",
    "    roc_v=train_eval(atnm=sf\n",
    "                     ,train=False\n",
    "                     ,opti=optimizer\n",
    "                     ,crit=criterion\n",
    "                     ,eval_metrics=True\n",
    "                     ,iterator=iter(val_dl)\n",
    "                    ,writer=writer\n",
    "                    ,n_iter=n_iter)\n",
    "    \n",
    "    eval_v,error_list,reason_wrong=explanation_score(sf,it=iter(val_dl))\n",
    "    \n",
    "    if len(error_list)>0 and len(reason_wrong)>0:\n",
    "        #In this list comprehension we check how many of the conditions we recovered (even if we got too many)\n",
    "        incorrect_predictions=[]\n",
    "        len_list=[]\n",
    "        summ=[]\n",
    "        for err,rea in zip(error_list,reason_wrong):\n",
    "            #if its zero, it means either: We have no reason (which adds to the )\n",
    "            if len(err)>0 and len(rea)>0:\n",
    "                le=np.unique(np.stack([x[0][0:2] for x in err]),axis=0)\n",
    "                #here we do the comparrison, if an elemen in the (reason(which is a  tuple) is in the reason list)\n",
    "                comp_lists=[m == [x[0][0:2] for x in  err] for m in rea]\n",
    "                #if it appears it will show two ones in that row, that means when we sum over the rows we get a 2 \n",
    "                #now we have to care about situation, where the releant input appears multiple times. also we need to check thaat\n",
    "                #each reason appears, that happens here\n",
    "                all_inputs_appear=np.sum([any(np.sum(x,axis=1)==2) for x in comp_lists])==len(comp_lists)\n",
    "                summ.append(all_inputs_appear)\n",
    "                len_list.append(le)\n",
    "                \n",
    "            #We didnt put weight on anything (happens at staart of training)\n",
    "            if len(err)==0:\n",
    "                summ.append(0)\n",
    "            if len(rea)==0:\n",
    "                incorrect_predictions.append((err,rea))\n",
    "\n",
    "        sensitivity=np.sum(summ)/len(summ)\n",
    "        \n",
    "    else:\n",
    "        sensitivity=0\n",
    "        \n",
    "    auc_list.append(roc_v)\n",
    "    totally_correct_list.append(eval_v)\n",
    "    sensitivity_list.append(sensitivity)\n",
    "    \n",
    "    writer.add_scalar(\"auc\",roc_v,n_iter)\n",
    "    writer.add_scalar(\"totally_correct\",eval_v,n_iter)\n",
    "    writer.add_scalar(\"sensitivtiy\",sensitivity,n_iter)\n",
    "    \n",
    "    print(roc_v)\n",
    "    print(eval_v)\n",
    "    print(sensitivity)\n",
    "    print(np.mean([len(x) for x in len_list]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([18, 13]), array([6, 1]), 0.6842163988189459]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  1.],\n",
       "       [18., 13.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list[112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[18.        , 13.        ,  0.57301956,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.46191886, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.09993429, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.53477967,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.38609537, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.39514545,  0.95999998]]),\n",
       "  array([[18.        , 13.        ,  0.09564076,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.01215715, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.68229145,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.71779233,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.47639892, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.85839802,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.12299316, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.16897102,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.02462346, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.62957352,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.62550533, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.89134288, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.33487678, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.61687964,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.46448573, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.44488743,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.33258575, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.54035747,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.7448644 ,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.96794271, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.63304895,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.23525831, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.07847972, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.02933228,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.47502831,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.88253456, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.17826097, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.85734731,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.29600477, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.12354995,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.37142733,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.46857855, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.05036784, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.06973726,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.36814719,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.77987885, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.10069015,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.20273294, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.86858666, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.37123755,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.42836374, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.15043795,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.17192964, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.58357942,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.53367275,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.96131778, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.57154733,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.87706935, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.0553115 ,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.092694  ,  0.94999999]])],\n",
       " [array([[6.00000000e+00, 1.00000000e+00, 4.06532381e-05, 9.70000029e-01]]),\n",
       "  array([[18.        , 13.        ,  0.2289793 ,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.49248847,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.94052362, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.01755623, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.83598053,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.90404332,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.41499498, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.17808487, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.51292384,  0.95999998]]),\n",
       "  array([[18.        , 13.        ,  0.18396413,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.94824553, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.97993702,  0.94      ]])],\n",
       " [array([[18.        , 13.        ,  0.80200022,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.78571516, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.42141891,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.82876039, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.43594909, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.98259622,  0.94      ]])],\n",
       " [array([[6.        , 1.        , 0.05585877, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.28969613,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.39566156, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.3948898 , 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.84147769,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.41070616, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.99108887,  0.94      ]])],\n",
       " [array([[18.        , 13.        ,  0.221901  ,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.14279097, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.37155569,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.833166  , 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.36200187,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.7356841 , 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.4023003 ,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.07595441, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.38527429,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.93038124,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.81085128, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.4101685 ,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.53903931, 0.97000003]])],\n",
       " [array([[1.80000000e+01, 1.30000000e+01, 1.26476744e-02, 9.49999988e-01]]),\n",
       "  array([[6.        , 1.        , 0.36489138, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.28087848, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.91167307,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.4048821 ,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.44040781, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.27670518, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.59498209,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.89749509, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.44949535,  0.95999998]]),\n",
       "  array([[18.        , 13.        ,  0.68330199,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.72096449,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.41715574, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.7005955 ,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.17041788, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.74537998, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.57956332,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.96527189,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.34179088,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.93170339, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.56106752, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.11240871,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.66742772,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.84134197, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.91546178,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.39882278, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.64253902,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.12381186, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.77997738,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.10934889, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.13986455,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.60915077, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.67033267, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.26290858,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.36048833, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.34562892, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.06469334,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.67767626, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.04795764,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.52199084,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.19542018,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.59512007, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.65442353,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.52425408,  0.95999998]]),\n",
       "  array([[6.00000000e+00, 1.00000000e+00, 3.91356507e-03, 9.70000029e-01]])],\n",
       " [array([[18.        , 13.        ,  0.85302532,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.83339435, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.22606978,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.78805768, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.07187293,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.39974946, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.70762426, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.24646023,  0.95999998]]),\n",
       "  array([[18.        , 13.        ,  0.96599054,  0.94      ]])],\n",
       " [array([[18.        , 13.        ,  0.89986014,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.62334615, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.53555638,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.70660156,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.32326519, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.61230129,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.67438114, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.75919068, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.5066933 ,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.61288857, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.68011165,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.62185431,  0.94999999]])],\n",
       " [array([[1.80000000e+01, 1.30000000e+01, 6.54887641e-03, 9.49999988e-01]]),\n",
       "  array([[6.        , 1.        , 0.75789404, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.60216463, 0.97000003]]),\n",
       "  array([[1.80000000e+01, 1.30000000e+01, 7.21794227e-03, 9.49999988e-01]])],\n",
       " [array([[6.        , 1.        , 0.60733438, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.32217854,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.36079782, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.73337489,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.61945301, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.53495127,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.43360424, 0.97000003]])],\n",
       " [array([[1.80000000e+01, 1.30000000e+01, 1.68529549e-03, 9.49999988e-01]]),\n",
       "  array([[6.        , 1.        , 0.10814946, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.43209535,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.94065034, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.5670225 , 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.91859859,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.24370188,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.00631274, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.40062159,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.3969937 , 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.20048225,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.18337511, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.73417592, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.57886088,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.81905884,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.47833288, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.36228839,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.05056594, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.89129204,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.64420861, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.94404078, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.65553612,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.31039488, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.86724287,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.31347817, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.136075  ,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.57229847,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.76907748, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.66412759, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.45100588,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.87419802,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.36844176, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.43678904, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.32200816,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.64978153, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.21119143,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.72994775, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.68153793,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.86076474,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.97809297, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.87181163,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.05763927,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.40504575, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.75007105,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.88609183, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.84906781,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.56939113, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.63000125,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.9426679 , 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.07163633, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.8450557 ,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.73103803,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.4599613 , 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.36993781,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.65710825, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.44523332, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.71403491,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.83884692, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.0379835 ,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.54450858, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.67338228,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.20140859, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.12224063,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.83124328, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.57040143, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.68898469,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.24123402,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.0621882 , 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.94794792,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.21150403, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.09367378,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.74027079,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.76987445, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.78341985, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.89155316, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.88266701,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.97476673, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.25791883,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.93749577, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.79275429,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.30467889,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.58917403, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.3173663 , 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.61287636, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.55684698,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.95599467,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.73334885, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.55109298,  0.95999998]]),\n",
       "  array([[18.        , 13.        ,  0.55055064,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.44322661, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.96491891,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.56155467, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.76791424, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.86229885,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.49005154, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.20428716,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.0908367 , 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.46322906,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.17893384, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.91781032,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.80401981, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.61854386,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.12809879, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.8433848 ,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.4274371 ,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.03650314, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.30928436,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.94636649, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.68772328,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.86866099, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.75439125,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.78091103, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.63125312,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.9613809 , 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.2031236 ,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.83727998, 0.97000003]]),\n",
       "  array([[1.80000000e+01, 1.30000000e+01, 1.29242791e-02, 9.49999988e-01]])],\n",
       " [array([[18.        , 13.        ,  0.66495389,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.7637949 , 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.03716355, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.9601661 ,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.88581681, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.68279576,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.77972865,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.11100676, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.49537158, 0.97000003]]),\n",
       "  array([[1.80000000e+01, 1.30000000e+01, 9.15314071e-03, 9.49999988e-01]])],\n",
       " [array([[18.        , 13.        ,  0.53180379,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.71356052, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.29183418, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.04821747,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.12036304, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.02986771,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.19387293, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.17869443,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.17991854, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.10966119, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.93370914,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.4917452 , 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.83704817,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.74713641, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.37698689,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.47516167, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.57858527,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.86646837, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.13108486,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.95532691, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.36009932,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.62083691,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.81170738, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.93463695, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.27500916,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.12578991,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.19246976, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.46617392, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.46363458,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.31314349, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.56790936,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.26467389, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.41685888,  0.95999998]]),\n",
       "  array([[18.        , 13.        ,  0.75907713,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.65596718,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.93500805, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.37027851, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.38044313,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.97707731, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.19155955, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.51029313,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.49214867, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.24485612,  0.95999998]])],\n",
       " [array([[6.        , 1.        , 0.41500694, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.42047039,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.68564868,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.71977699, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.92645246, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.47967079,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.93046242,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.29874492, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.35654071,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.58856428, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.19930497, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.56598902, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.54076469,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.19065051,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.46491158, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.83112103,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.0770304 , 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.06685107, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.31675616,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.11070868,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.33762988, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.67285037, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.07407952,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.26665726,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.84618002, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.66098875, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.74988782,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.19487321,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.70664579, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.13577306,  0.94999999]]),\n",
       "  array([[18.        , 13.        ,  0.27510795,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.8245818 , 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.39058489, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.64800191,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.33425289, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.28992888,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.93798763,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.89670545, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.70908433, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.23039635,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.77631021,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.35399249, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.62854207,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.44856346, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.91282469,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.26173931, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.98693258,  0.94      ]]),\n",
       "  array([[6.        , 1.        , 0.08231337, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.21490611,  0.95999998]]),\n",
       "  array([[18.        , 13.        ,  0.87289   ,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.60086375, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.40046668, 0.97000003]]),\n",
       "  array([[6.        , 1.        , 0.34748071, 0.97000003]]),\n",
       "  array([[1.80000000e+01, 1.30000000e+01, 1.44199366e-02, 9.49999988e-01]])],\n",
       " [array([[18.        , 13.        ,  0.62588745,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.47750068, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.93101239, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.82457113,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.13863516, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.12808584,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.29544774, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.65444475,  0.94999999]])],\n",
       " [array([[6.        , 1.        , 0.14097016, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.75755072,  0.94999999]])],\n",
       " [array([[18.        , 13.        ,  0.93947065,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.56899691, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.93105072,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.831945  , 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.88805294,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.76265204, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.11690339,  0.94999999]]),\n",
       "  array([[6.        , 1.        , 0.41101193, 0.97000003]])],\n",
       " [array([[18.        , 13.        ,  0.33449095,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.02820047, 0.97000003]])],\n",
       " [array([[6.        , 1.        , 0.93462396, 0.97000003]]),\n",
       "  array([[18.        , 13.        ,  0.57803226,  0.95999998]])],\n",
       " [array([[18.        , 13.        ,  0.36949822,  0.95999998]]),\n",
       "  array([[6.        , 1.        , 0.41791925, 0.97000003]])]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
