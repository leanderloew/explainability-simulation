{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo of Self Attention in Pytorch \n",
    "\n",
    "In this Notebook we want to test, that Self-Attention and Piece wise Feed Forward Neural Networks can solve a simple logical, deterministic problem. Furthermore, we want to attest to the degree, that the attention weights can \"explain\" the predictions. We see that this is the case for Piece Wise Feed Forward Neural Netowrks but it is not! the case for Self Attention Neural Networks. We conjegture, that this is due to the fact that the representation of Self Attention is dependend on all elements of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from model_pytorch import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,sampler,DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "batch_size=2048\n",
    "max_epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we set up a logical function \n",
    "def add_1(a,b,v,rand_list):\n",
    "    #If i do it this way the ordereing doenst matter !!!! ( have to modify it)\n",
    "    places_a=np.sum(rand_list[:,0:2]==a,axis=1)==2\n",
    "    places_b=np.sum(rand_list[:,0:2]==b,axis=1)==2\n",
    "    #not using the value right now\n",
    "    \n",
    "    if (any(places_a) and any(places_b)): \n",
    "        #and any(rand_list[places_a][:,2]>v) and any(rand_list[places_b][:,2]>v)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy_data(Dataset):\n",
    "    def __init__(self, input,output,reason):     \n",
    "        \n",
    "        self.data=input\n",
    "        self.label=output\n",
    "        self.reason=reason\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        X=self.data[index]\n",
    "        y=self.label[index]\n",
    "        r=self.reason[index]\n",
    "        return X, y, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THe collate difnes how data gets batched \n",
    "#here we are using a simple dynamic badding for batching. \n",
    "def my_collate(batch):\n",
    "    texts=([x[0] for x in batch])\n",
    "    labels=np.array(([x[1] for x in batch]))\n",
    "    reason=np.array(([x[2] for x in batch]))\n",
    "    batch_size=len(batch)\n",
    "    maxlen=np.max([len(x) for x in texts])\n",
    "    text_stack=np.zeros(shape=(batch_size,maxlen,3))\n",
    "    for enu,txt in enumerate(texts):\n",
    "        text_stack[enu,0:len(txt),:]=txt\n",
    "\n",
    "    return torch.tensor(text_stack[:,:,0]).cuda(),torch.tensor(text_stack[:,:,1]).cuda(),torch.tensor(text_stack[:,:,2]).cuda(),torch.tensor(labels).cuda(),reason\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_fraud_model(nn.Module):\n",
    "    def __init__(self, d_model=32,heads=1,nlay=1,dropout=0,SelfA=True,return_w=False):\n",
    "        \n",
    "        super(simple_fraud_model,self).__init__()\n",
    "        self.return_w=return_w\n",
    "        emb_d_1=int(d_model/2)\n",
    "        emb_d_2=int(d_model/2)\n",
    "        \n",
    "        self.embedding_1=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_1)\n",
    "        self.embedding_2=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_2-1)\n",
    "\n",
    "        \n",
    "        if SelfA==True:\n",
    "            self.encoder_layers=EncoderLayer(d_model=d_model,heads=heads,dropout=dropout,share_params=True)\n",
    "\n",
    "        if SelfA==False:\n",
    "            self.encoder_layers=FeedForward(d_model)\n",
    "\n",
    "\n",
    "        self.mula=multi_attention(input_dim=d_model,key_dim=d_model,nheads=1,return_weights=True,value_dim=d_model)\n",
    "\n",
    "        self.fully_con=nn.Linear(d_model,d_model*4)\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        self.fully_con_1=nn.Linear(d_model*4,d_model*4)\n",
    "        self.relu_1=nn.ReLU()\n",
    "        \n",
    "        self.final_fully_con=nn.Linear(d_model*4,1)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.selfa=SelfA\n",
    "        \n",
    "        \n",
    "    def forward(self, x1,x2,x3,return_w):\n",
    "        e1=self.embedding_1(x1)\n",
    "        e2=self.embedding_2(x2)\n",
    "\n",
    "        cat=torch.cat([e1,e2,x3.unsqueeze(2)],dim=2)\n",
    "        if self.selfa==True:\n",
    "            \n",
    "            feat,w2=self.encoder_layers(cat)\n",
    "        else:\n",
    "            feat=self.encoder_layers(cat)\n",
    "\n",
    "        ag,weights=self.mula(feat)\n",
    "        #first layer with a relu activation\n",
    "        fc=self.relu(self.fully_con(ag.squeeze()))\n",
    "        #second layer\n",
    "        fc=self.relu_1(self.fully_con_1(fc))\n",
    "        #finally the last layer with a sigmoid\n",
    "        preds=self.sig(self.final_fully_con(fc))\n",
    "        \n",
    "        if return_w==False:\n",
    "            return preds.squeeze()\n",
    "        if return_w==True:\n",
    "            \n",
    "            if self.selfa==True:\n",
    "                #this might not be a 100% correct\n",
    "                #We are multiplying the first layer attention weights together with the aggregation weights. \n",
    "                return preds.squeeze(),weights#torch.bmm(w2.squeeze(),weights)\n",
    "            else:\n",
    "                return preds.squeeze(),weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A train and evaluation funciton\n",
    "def train_eval(atnm,train,opti,crit,eval_metrics,iterator,n_iter,writer):\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "    atnm: A Model to be trained/evalued\n",
    "    train: If we want to train/eval (If train we sub the gradient)\n",
    "    opti: An optimizer to be used\n",
    "    crit: A loss function to be used\n",
    "    eval_matrics: If we want to keep track of predictions during batch gen and in the end\n",
    "    calculate a metric on the whole data (aka the AUC)\n",
    "    iterator: The data generator as an iterator\n",
    "    n_iter: the current step to be updated\n",
    "    writer: the tensorboard writer used to keep trakc of training results\n",
    "    \n",
    "    '''\n",
    "    if train==False:\n",
    "        loss_val=[]\n",
    "        name1=\"val_loss\"\n",
    "        name2=\"val_roc\"  \n",
    "        atnm.eval()\n",
    "    else:\n",
    "        name1=\"train_loss\"\n",
    "        name2=\"train_roc\"\n",
    "        atnm.train()\n",
    "\n",
    "    if eval_metrics:\n",
    "        store_label=[]\n",
    "        store_preds=[]\n",
    "        \n",
    "    #The epcoh \n",
    "    for batch in iterator:\n",
    "        #print(batch[0].shape[1])\n",
    "        opti.zero_grad()\n",
    "        #batch=ba\n",
    "        predictions,w=atnm(batch[0].long(),batch[1].long(),batch[2].float(),True)        #print(predictions)\n",
    "        loss = crit(predictions, batch[3].float().cuda())\n",
    "\n",
    "        if train==True:\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            n_iter=n_iter+1\n",
    "            writer.add_scalar(name1,loss.cpu().detach().numpy(),n_iter)\n",
    "        #when we dont train we dont write during epoch but only at the end\n",
    "        #also we dont up the iter\n",
    "        if train==False:\n",
    "            loss_val.append(loss.cpu().detach().numpy())\n",
    "        if eval_metrics== True: \n",
    "            store_preds.append(predictions.cpu().detach().numpy())\n",
    "            store_label.append(batch[3].float().cpu().detach().numpy())\n",
    "            \n",
    "        del predictions\n",
    "        del loss\n",
    "        \n",
    "    ### End of Batch\n",
    "    if train == False:\n",
    "        writer.add_scalar(name1,np.mean(loss_val),n_iter)\n",
    "\n",
    "    if eval_metrics== True: \n",
    "        store_preds=np.concatenate(store_preds)\n",
    "        store_label=np.concatenate(store_label)\n",
    "        roc=roc_auc_score(store_label,store_preds)\n",
    "        writer.add_scalar(name2,roc,n_iter)\n",
    "        \n",
    "        return roc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_score(sf,it):\n",
    "    reason_list=[]\n",
    "    explanation_list=[]\n",
    "    weight_list=[]\n",
    "    sf.eval()\n",
    "    \n",
    "    for batch in it:\n",
    "\n",
    "        #We get both inputs\n",
    "        i1=batch[0].long()\n",
    "        i2=batch[1].long()\n",
    "        i3=batch[2].float()\n",
    "\n",
    "        #this is directly a list \n",
    "        r=batch[4]\n",
    "\n",
    "        #We get both prediction and attention weights \n",
    "        predictions,weights=sf(i1,i2,batch[2].float(),True)\n",
    "\n",
    "        #Get the predictions\n",
    "        preds_np=np.round(predictions.detach().cpu().numpy())\n",
    "\n",
    "        #get the attention weights \n",
    "        weights_np=weights.detach().cpu().numpy()\n",
    "        fraud_elem=np.where(preds_np>0.5)[0]\n",
    "\n",
    "        #Only if we actually predict something we care about an explanation (we can change that threshhold)\n",
    "        if fraud_elem.size>0:\n",
    "            \n",
    "            i1=i1.detach().cpu().numpy()\n",
    "            i2=i2.detach().cpu().numpy()\n",
    "            i3=i3.detach().cpu().numpy()\n",
    "\n",
    "            #This is the reason\n",
    "            for fraud_ele in fraud_elem:\n",
    "\n",
    "                #get the weigths(we round casue we want to look at them later)\n",
    "                my_weight=np.squeeze(np.round(weights_np[fraud_ele],2))\n",
    "\n",
    "                #get the element in the input the model put attention on\n",
    "                atn_elem=np.where(my_weight>0.5)[0]\n",
    "\n",
    "                #get the actual value so we can check if the actual confidence is a good measure\n",
    "                atn_value=np.expand_dims(my_weight[np.where(my_weight>0.5)[0]],axis=1)\n",
    "\n",
    "                #Next we stack up all inputs \n",
    "                in_elem=np.stack([np.squeeze(i1[fraud_ele]),np.squeeze(i2[fraud_ele]),np.squeeze(i3[fraud_ele])],axis=1)\n",
    "\n",
    "                #we get the input elements our model has picked out \n",
    "                in_elem_j=in_elem[atn_elem]\n",
    "            \n",
    "                #then we concatenate together with the other variables\n",
    "                in_elem_j=np.concatenate([in_elem_j,atn_value],axis=1)\n",
    "\n",
    "                #in the end we save everything in a list for evaluation\n",
    "                explanation_list.append(in_elem_j)\n",
    "                reason_list.append(r[fraud_ele])\n",
    "                \n",
    "                \n",
    "    if (len(reason_list))>0 and (len(explanation_list))>0:\n",
    "        error_list=[]\n",
    "        reason_wrong=[]\n",
    "\n",
    "        for explanation,reason in zip(explanation_list,reason_list):\n",
    "            e=explanation\n",
    "            r=reason\n",
    "            if len(e)>0:\n",
    "                splits=np.split(e[:,0:4],axis=0,indices_or_sections=len(e))\n",
    "\n",
    "                if len(r)>0:\n",
    "                    e=np.array([j[0][0:2] for j in splits])\n",
    "                    #new_array = [tuple(row) for row in e]\n",
    "                    #e = np.unique(new_array)\n",
    "\n",
    "                    e=np.unique(e,axis=0)\n",
    "\n",
    "                    r=np.array(r[0][0:2])\n",
    "\n",
    "                    #if not np.array_equal(np.sort(r.flatten()),np.sort(e.flatten())):\n",
    "                    error_list.append(splits)\n",
    "                    reason_wrong.append(r)\n",
    "                else:\n",
    "                    error_list.append(splits)\n",
    "                    reason_wrong.append(r)\n",
    "            else:\n",
    "                error_list.append(e)\n",
    "                reason_wrong.append(r)        \n",
    "                    #print(reason_wrong)\n",
    "        #print(reason_list)\n",
    "        #so we define it as an error if we did not return the exact two conditions that lead to a \"fraud\" in a particular claim \n",
    "        error_score=len(error_list)/len(reason_list)\n",
    "        \n",
    "        return error_score,error_list,reason_wrong\n",
    "    else:\n",
    "        return 1,[],[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lists=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we generate the data we will store:\n",
    "#first we get a set of fraud elements (a pair of numbers and a numeric value)\n",
    "for n_fraud in range(5,20):\n",
    "    \n",
    "    for j in range(36):\n",
    "        fraud_list=[]\n",
    "        wait=0\n",
    "\n",
    "        for j in range(n_fraud):\n",
    "            fraud_elem=[np.array([random.randint(1,20),random.randint(1,20)]),np.array([random.randint(1,20),random.randint(1,20)]),random.random()]\n",
    "            fraud_list.append(fraud_elem)\n",
    "\n",
    "        #All the input data present\n",
    "        all_data=[]\n",
    "        #if at least one of the \"reasons\" was present\n",
    "        all_label=[]\n",
    "        #and the actual reason present ( we can use that to check if the explanation was correct)\n",
    "        all_reason=[]\n",
    "\n",
    "        for j in range(100000):\n",
    "            rand_list=[]\n",
    "            for j in range(random.randint(1,100)):\n",
    "                rand_list.append((random.randint(1,20),random.randint(1,20),random.random()))\n",
    "\n",
    "            rand_list=np.array(rand_list)\n",
    "\n",
    "            add_list=[]\n",
    "            reason_list=[]\n",
    "\n",
    "            for i in fraud_list:\n",
    "                if add_1(a=i[0],b=i[1],v=i[2],rand_list=rand_list):\n",
    "\n",
    "                    add_list.append(True)\n",
    "                    reason_list.append(i)\n",
    "\n",
    "                else:\n",
    "                    add_list.append(False)\n",
    "            if any(add_list):\n",
    "                all_label.append(1) \n",
    "            else:\n",
    "                all_label.append(0)\n",
    "\n",
    "            all_data.append(rand_list)\n",
    "            all_reason.append(reason_list)\n",
    "        test_share=0.1\n",
    "        train_split_elem=int((1-test_share)*len(all_data))\n",
    "\n",
    "        # we split into train and validation\n",
    "\n",
    "        train_X=all_data[:train_split_elem]\n",
    "        val_X=all_data[train_split_elem:]\n",
    "\n",
    "        train_y=all_label[:train_split_elem]\n",
    "        val_y=all_label[train_split_elem:]\n",
    "\n",
    "        train_r=all_reason[:train_split_elem]\n",
    "        val_r=all_reason[train_split_elem:]\n",
    "\n",
    "        train_ds=dummy_data(train_X,train_y,train_r)\n",
    "        val_ds=dummy_data(val_X,val_y,val_r)\n",
    "        val_dl= DataLoader(dataset=val_ds,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=my_collate\n",
    "                              )\n",
    "\n",
    "        train_dl= DataLoader(dataset=train_ds,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=my_collate\n",
    "                              )\n",
    "\n",
    "\n",
    "        sf=simple_fraud_model(SelfA=False,d_model=32).cuda()\n",
    "        criterion=nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(sf.parameters(), lr=5e-4)\n",
    "        scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\"max\")\n",
    "\n",
    "        writer = SummaryWriter(log_dir=\"logs/pff_{}\".format(n_fraud))\n",
    "\n",
    "        sensitivity_list=[]\n",
    "        totally_correct_list=[]\n",
    "        auc_list=[]\n",
    "        lens_list=[]\n",
    "\n",
    "        for j in range(max_epochs):\n",
    "            #atnm,train,opti,crit,eval_metrics,iterator\n",
    "            n_iter=(train_ds.__len__()/batch_size)*j\n",
    "\n",
    "            roc_t=train_eval(sf,True,optimizer,criterion,True\n",
    "                           ,iter(train_dl),n_iter=n_iter,writer=writer)\n",
    "\n",
    "            roc_v=train_eval(atnm=sf\n",
    "                             ,train=False\n",
    "                             ,opti=optimizer\n",
    "                             ,crit=criterion\n",
    "                             ,eval_metrics=True\n",
    "                             ,iterator=iter(val_dl)\n",
    "                            ,writer=writer\n",
    "                            ,n_iter=n_iter)\n",
    "\n",
    "            eval_v,error_list,reason_wrong=explanation_score(sf,it=iter(val_dl))\n",
    "\n",
    "            if len(error_list)>0 and len(reason_wrong)>0:\n",
    "                #In this list comprehension we check how many of the conditions we recovered (even if we got too many)\n",
    "                incorrect_predictions=[]\n",
    "                len_list=[]\n",
    "                summ=[]\n",
    "                for err,rea in zip(error_list,reason_wrong):\n",
    "                    #if its zero, it means either: We have no reason (which adds to the )\n",
    "                    if len(err)>0 and len(rea)>0:\n",
    "                        le=np.unique(np.stack([x[0][0:2] for x in err]),axis=0)\n",
    "                        #here we do the comparrison, if an elemen in the (reason(which is a  tuple) is in the reason list)\n",
    "                        comp_lists=[m == [x[0][0:2] for x in  err] for m in rea]\n",
    "                        #if it appears it will show two ones in that row, that means when we sum over the rows we get a 2 \n",
    "                        #now we have to care about situation, where the releant input appears multiple times. also we need to check thaat\n",
    "                        #each reason appears, that happens here\n",
    "                        all_inputs_appear=np.sum([any(np.sum(x,axis=1)==2) for x in comp_lists])==len(comp_lists)\n",
    "                        summ.append(all_inputs_appear)\n",
    "                        len_list.append(le)\n",
    "\n",
    "                    #We didnt put weight on anything (happens at staart of training)\n",
    "                    if len(err)==0:\n",
    "                        summ.append(0)\n",
    "                    if len(rea)==0:\n",
    "                        incorrect_predictions.append((err,rea))\n",
    "\n",
    "                sensitivity=np.sum(summ)/len(summ)\n",
    "\n",
    "            else:\n",
    "                sensitivity=0\n",
    "\n",
    "            auc_list.append(roc_v)\n",
    "            sensitivity_list.append(sensitivity)\n",
    "\n",
    "            writer.add_scalar(\"auc\",roc_v,n_iter)\n",
    "            writer.add_scalar(\"sensitivtiy\",sensitivity,n_iter)\n",
    "\n",
    "            print(roc_v)\n",
    "            print(sensitivity)\n",
    "            if len(error_list)>0 and len(reason_wrong)>0:\n",
    "                lens=np.mean([len(x) for x in len_list])\n",
    "                print(lens)\n",
    "                writer.add_scalar(\"length_ones\",lens,n_iter)\n",
    "                lens_list.append(lens)\n",
    "                best_len=np.min(lens_list)\n",
    "\n",
    "            best_auc=np.max(auc_list)\n",
    "            best_sensi=np.max(sensitivity_list)\n",
    "            scheduler.step(roc_v)\n",
    "\n",
    "            if best_auc==1:\n",
    "                wait=wait+1\n",
    "                if wait==10:\n",
    "                    break\n",
    "\n",
    "            if roc_v<best_auc:\n",
    "                wait=wait+1\n",
    "                if wait==10:\n",
    "                    break\n",
    "        best_lists.append([best_auc,best_sensi,best_len])\n",
    "        \n",
    "    best_n_list.append(best_lists)\n",
    "\n",
    "    with open('result_list_36_20_2.pickle', 'wb') as handle:\n",
    "        pickle.dump(best_lists, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_frames=np.array_split(np.array(best_lists),indices_or_sections=9,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(splitted_frames, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_frame=splitted_frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leander_low/.conda/envs/pytorch_env/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: Mean of empty slice\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mean_list=[]\n",
    "mean_best_list=[]\n",
    "wperfect_list=[]\n",
    "for my_frame in splitted_frames:\n",
    "    mean_results=np.nanmean(my_frame,axis=0,)\n",
    "    where_perfect=np.where(my_frame[:,0]==1)[0]\n",
    "    mean_perfect=np.nanmean(my_frame[where_perfect],axis=0)\n",
    "    mean_list.append(mean_results)\n",
    "    mean_best_list.append(mean_perfect)\n",
    "    wperfect_list.append(len(where_perfect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 6, 9, 5, 2, 3, 0, 0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wperfect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_frame=splitted_frames[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results=np.mean(my_frame,axis=0)\n",
    "where_perfect=np.where(my_frame[:,0]==1)[0]\n",
    "mean_perfect=np.mean(my_frame[where_perfect],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 2.]),\n",
       " array([1.        , 1.        , 2.27636881]),\n",
       " array([0.99486063, 0.99693681, 2.53852556]),\n",
       " array([0.99911283, 1.        , 2.85391878]),\n",
       " array([0.99583798, 0.99165575, 3.08948516]),\n",
       " array([0.98654894, 0.98857062, 3.29730064]),\n",
       " array([0.99328035, 0.9900803 , 3.62815605]),\n",
       " array([0.98355022, 0.99250612, 3.23018346]),\n",
       " array([0.9912021 , 0.98298721, 4.21266864])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 2.]),\n",
       " array([1.        , 1.        , 2.27636881]),\n",
       " array([1.        , 1.        , 2.58934967]),\n",
       " array([1.        , 1.        , 2.85821124]),\n",
       " array([1.        , 1.        , 3.16450371]),\n",
       " array([1.        , 1.        , 3.53490882]),\n",
       " array([1.        , 1.        , 3.67902599]),\n",
       " array([nan, nan, nan]),\n",
       " array([nan, nan, nan])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_best_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.],\n",
       "        [1., 1., 2.]]), array([[1.        , 1.        , 2.20727273],\n",
       "        [1.        , 1.        , 2.25252525],\n",
       "        [1.        , 1.        , 2.30944625],\n",
       "        [1.        , 1.        , 2.28614458],\n",
       "        [1.        , 1.        , 2.30351438],\n",
       "        [1.        , 1.        , 2.25631769],\n",
       "        [1.        , 1.        , 2.31920904],\n",
       "        [1.        , 1.        , 2.33448276],\n",
       "        [1.        , 1.        , 2.31692308],\n",
       "        [1.        , 1.        , 2.17785235]]), array([[0.98873954, 0.98130841, 2.45686901],\n",
       "        [0.98726427, 0.9880597 , 2.45047923],\n",
       "        [1.        , 1.        , 2.60373444],\n",
       "        [1.        , 1.        , 2.55813953],\n",
       "        [0.98467984, 1.        , 2.50759878],\n",
       "        [1.        , 1.        , 2.57731959],\n",
       "        [1.        , 1.        , 2.59791667],\n",
       "        [1.        , 1.        , 2.60706861],\n",
       "        [0.9879227 , 1.        , 2.43421053],\n",
       "        [1.        , 1.        , 2.59191919]]), array([[1.        , 1.        , 2.92857143],\n",
       "        [1.        , 1.        , 2.71594684],\n",
       "        [1.        , 1.        , 2.94224422],\n",
       "        [1.        , 1.        , 2.88994307],\n",
       "        [1.        , 1.        , 2.7661597 ],\n",
       "        [1.        , 1.        , 2.86583184],\n",
       "        [1.        , 1.        , 2.83012259],\n",
       "        [1.        , 1.        , 2.87001733],\n",
       "        [1.        , 1.        , 2.9150641 ],\n",
       "        [0.99112831, 1.        , 2.81528662]]), array([[1.        , 1.        , 3.1976912 ],\n",
       "        [1.        , 1.        , 3.23338048],\n",
       "        [1.        , 1.        , 3.19393939],\n",
       "        [1.        , 1.        , 3.02479339],\n",
       "        [0.99195862, 1.        , 2.99835796],\n",
       "        [0.99280864, 0.97189349, 3.03949447],\n",
       "        [1.        , 1.        , 3.17271408],\n",
       "        [0.97364621, 0.94466403, 2.89183223],\n",
       "        [1.        , 1.        , 2.99562682],\n",
       "        [0.99996631, 1.        , 3.14702155]]), array([[1.        , 1.        , 3.58488613],\n",
       "        [0.99377642, 1.        , 3.29230769],\n",
       "        [0.99386241, 1.        , 3.37772087],\n",
       "        [0.9547885 , 0.97907324, 3.0048    ],\n",
       "        [1.        , 1.        , 3.48493151],\n",
       "        [0.97888794,        nan,        nan],\n",
       "        [0.98117045, 0.95558959, 3.10060976],\n",
       "        [0.97534401, 0.98253968, 3.22380952],\n",
       "        [0.99385004, 0.97993311, 3.30105018],\n",
       "        [0.99380967, 1.        , 3.30559006]]), array([[0.98176939, 0.97044917, 3.39620403],\n",
       "        [1.        , 1.        , 3.80515464],\n",
       "        [0.99989655, 1.        , 3.83806519],\n",
       "        [1.        , 1.        , 3.6037182 ],\n",
       "        [0.99999624, 1.        , 3.6366171 ],\n",
       "        [0.96285973, 0.96567164, 3.25263158],\n",
       "        [0.99398394, 0.96468214, 3.6807152 ],\n",
       "        [0.99999858, 1.        , 3.806     ],\n",
       "        [1.        , 1.        , 3.62820513],\n",
       "        [0.99429911, 1.        , 3.63424947]]), array([[0.93855382, 0.98465473, 1.        ],\n",
       "        [0.99482815,        nan,        nan],\n",
       "        [0.99998961, 1.        , 4.10671937],\n",
       "        [0.9999722 , 1.        , 4.079459  ],\n",
       "        [0.98418592, 0.95539419, 3.81541802],\n",
       "        [0.99994944, 1.        , 3.94399277],\n",
       "        [0.99971529, 1.        , 4.13425549],\n",
       "        [0.9999882 , 1.        , 3.76162299],\n",
       "        [0.91834227, 1.        , 1.        ],\n",
       "        [0.99997732,        nan,        nan]]), array([[0.99970535, 1.        , 4.32459522],\n",
       "        [0.99613611, 0.99081164, 4.06707787],\n",
       "        [0.99994367, 1.        , 4.4168    ],\n",
       "        [0.98685327, 0.97084806, 4.17253839],\n",
       "        [0.95533499, 0.93173653, 3.97964072],\n",
       "        [0.99482505, 0.99920319, 4.27377049],\n",
       "        [0.99997551, 1.        , 4.29937792],\n",
       "        [0.98575256, 0.93727273, 3.95545455],\n",
       "        [0.9999464 , 1.        , 4.33056604],\n",
       "        [0.99354813, 1.        , 4.30686518]])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch environment",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
