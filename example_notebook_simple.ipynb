{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo of Self Attention in Pytorch \n",
    "\n",
    "In this Notebook we want to test, that Self-Attention and Piece wise Feed Forward Neural Networks can solve a simple logical, deterministic problem. Furthermore, we want to attest to the degree, that the attention weights can \"explain\" the predictions. We see that this is the case for Piece Wise Feed Forward Neural Netowrks but it is not! the case for Self Attention Neural Networks. We conjegture, that this is due to the fact that the representation of Self Attention is dependend on all elements of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from model_pytorch import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,sampler,DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We generate list of range \n",
    "all_data=[]\n",
    "all_label=[]\n",
    "for j in range(100000):\n",
    "    rand_list=[]\n",
    "    for j in range(random.randint(1,100)):\n",
    "        rand_list.append((random.randint(1,20),random.randint(1,20),random.random()))\n",
    "    \n",
    "    rand_list=np.array(rand_list)\n",
    "    \n",
    "    places_17_1=np.sum(rand_list[:,0:2]==[17, 1],axis=1)==2\n",
    "    places_18_9=np.sum(rand_list[:,0:2]==[18, 9],axis=1)==2\n",
    "     \n",
    "    \n",
    "    if (any(places_17_1) and any(places_18_9) and any(rand_list[places_17_1][:,2]>0.5)and any(rand_list[places_18_9][:,2]<0.5)):\n",
    "        #We include this for mislabeling \n",
    "        all_label.append(1) \n",
    "    else:\n",
    "        all_label.append(0)\n",
    "        \n",
    "    all_data.append(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,val_X,train_y,val_y = train_test_split(all_data, all_label, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy_data(Dataset):\n",
    "    def __init__(self, input,output):     \n",
    "        \n",
    "        self.data=input\n",
    "        self.label=output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        X=self.data[index]\n",
    "        y=self.label[index]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=dummy_data(train_X,train_y)\n",
    "val_ds=dummy_data(val_X,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THe collate deffines how data gets batched \n",
    "\n",
    "def my_collate(batch):\n",
    "    \n",
    "    texts=([x[0] for x in batch])\n",
    "    labels=np.array(([x[1] for x in batch]))\n",
    "    batch_size=len(batch)\n",
    "    maxlen=np.max([len(x) for x in texts])\n",
    "    text_stack=np.zeros(shape=(batch_size,maxlen,3))\n",
    "    \n",
    "    for enu,txt in enumerate(texts):\n",
    "        text_stack[enu,0:len(txt),:]=txt\n",
    "\n",
    "    return torch.tensor(text_stack[:,:,0]).cuda(),torch.tensor(text_stack[:,:,1]).cuda(),torch.tensor(text_stack[:,:,2]).cuda(),torch.tensor(labels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl= DataLoader(dataset=val_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=False,\n",
    "                      collate_fn=my_collate\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl= DataLoader(dataset=train_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      collate_fn=my_collate\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_fraud_model(nn.Module):\n",
    "    def __init__(self, d_model=32,heads=1,nlay=1,dropout=0,SelfA=True,return_w=False):\n",
    "        \n",
    "        super(simple_fraud_model,self).__init__()\n",
    "        self.return_w=return_w\n",
    "        emb_d_1=int(d_model/2)\n",
    "        emb_d_2=int(d_model/2)\n",
    "        \n",
    "        self.embedding_1=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_1)\n",
    "        self.embedding_2=nn.Embedding(num_embeddings=21,embedding_dim=emb_d_2-1)\n",
    "\n",
    "        \n",
    "        if SelfA==True:\n",
    "            self.encoder_layers=EncoderLayer(d_model=d_model,heads=heads,dropout=dropout,share_params=True)\n",
    "\n",
    "        if SelfA==False:\n",
    "            self.encoder_layers=FeedForward(d_model)\n",
    "\n",
    "\n",
    "        self.mula=multi_attention(input_dim=d_model,key_dim=d_model,nheads=1,return_weights=True,value_dim=d_model)\n",
    "\n",
    "        self.fully_con=nn.Linear(d_model,d_model*2)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.final_fully_con=nn.Linear(d_model*2,1)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.selfa=SelfA\n",
    "        \n",
    "        \n",
    "    def forward(self, x1,x2,x3,return_w):\n",
    "        \n",
    "        e1=self.embedding_1(x1)\n",
    "        e2=self.embedding_2(x2)\n",
    "\n",
    "        cat=torch.cat([e1,e2,x3.unsqueeze(2)],dim=2)\n",
    "        if self.selfa==True:\n",
    "            \n",
    "            feat,w2=self.encoder_layers(cat)\n",
    "        else:\n",
    "            feat=self.encoder_layers(cat)\n",
    "\n",
    "        ag,weights=self.mula(feat)\n",
    "        fc=self.relu(self.fully_con(ag.squeeze()))\n",
    "        preds=self.sig(self.final_fully_con(fc))\n",
    "        if return_w==False:\n",
    "            return preds.squeeze()\n",
    "        if return_w==True:\n",
    "            if self.selfa==True:\n",
    "                #w2=w2.squeeze()\n",
    "                #w2=w2.permute((0,2,1))\n",
    "                #weights=torch.bmm(w2,weights)\n",
    "                return preds.squeeze(),torch.bmm(w2.squeeze(),weights)\n",
    "            else:\n",
    "                return preds.squeeze(),weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=simple_fraud_model(d_model=64,SelfA=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9525, 0.8354, 0.9743, 0.8289, 0.9388, 0.9793, 0.9597, 0.9782, 0.8659,\n",
       "        0.9772, 0.9597, 0.9660, 0.8520, 0.8799, 0.9687, 0.8247, 0.9273, 0.9750,\n",
       "        0.9523, 0.9218, 0.9754, 0.9797, 0.7633, 0.8099, 0.9723, 0.7238, 0.9753,\n",
       "        0.9696, 0.9827, 0.8308, 0.9766, 0.9805], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf(batch[0].long(),batch[1].long(),batch[2].float(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A train and evaluation funciton\n",
    "def train_eval(atnm,train,opti,crit,eval_metrics,iterator,n_iter,writer):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "    atnm: A Model to be trained/evalued\n",
    "    train: If we want to train/eval (If train we sub the gradient)\n",
    "    opti: An optimizer to be used\n",
    "    crit: A loss function to be used\n",
    "    eval_matrics: If we want to keep track of predictions during batch gen and in the end\n",
    "    calculate a metric on the whole data (aka the AUC)\n",
    "    iterator: The data generator as an iterator\n",
    "    n_iter: the current step to be updated\n",
    "    writer: the tensorboard writer used to keep trakc of training results\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if train==False:\n",
    "        loss_val=[]\n",
    "        name1=\"val_loss\"\n",
    "        name2=\"val_roc\"  \n",
    "        atnm.eval()\n",
    "    else:\n",
    "        name1=\"train_loss\"\n",
    "        name2=\"train_roc\"\n",
    "        atnm.train()\n",
    "\n",
    "    if eval_metrics:\n",
    "        store_label=[]\n",
    "        store_preds=[]\n",
    "    #The epcoh \n",
    "    for batch in iterator:\n",
    "        #print(batch[0].shape[1])\n",
    "        opti.zero_grad()\n",
    "        #batch=ba\n",
    "        predictions=atnm(batch[0].long(),batch[1].long(),batch[2].float(),False)        #print(predictions)\n",
    "        loss = crit(predictions, batch[3].float().cuda())\n",
    "\n",
    "        if train==True:\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            n_iter=n_iter+1\n",
    "            writer.add_scalar(name1,loss.cpu().detach().numpy(),n_iter)\n",
    "        #when we dont train we dont write during epoch but only at the end\n",
    "        #also we dont up the iter\n",
    "        if train==False:\n",
    "            loss_val.append(loss.cpu().detach().numpy())\n",
    "        if eval_metrics== True: \n",
    "            store_preds.append(predictions.cpu().detach().numpy())\n",
    "            store_label.append(batch[3].float().cpu().detach().numpy())\n",
    "            \n",
    "        del predictions\n",
    "        del loss\n",
    "### End of Batch\n",
    "    if train == False:\n",
    "        writer.add_scalar(name1,np.mean(loss_val),n_iter)\n",
    "\n",
    "    if eval_metrics== True: \n",
    "        store_preds=np.concatenate(store_preds)\n",
    "        store_label=np.concatenate(store_label)\n",
    "        roc=roc_auc_score(store_label,store_preds)\n",
    "        writer.add_scalar(name2,roc,n_iter)\n",
    "\n",
    "        return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set up a model and we use a BCE Loss and Adam optimizer\n",
    "sf=simple_fraud_model(SelfA=False,d_model=32).cuda()\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(sf.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can use tensorboard to keep track of the training process \n",
    "writer = SummaryWriter(log_dir=\"logs/pff_noresidual__:____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7506197943439853\n",
      "0.7677331365672024\n",
      "0.7794366747535523\n",
      "0.7938223528095131\n",
      "0.8337414713073631\n",
      "0.867031113960595\n",
      "0.8999413874667814\n",
      "0.9423927950211592\n",
      "0.9793395562519087\n",
      "0.988203516372624\n",
      "0.9903469520534304\n",
      "0.9917676895091153\n",
      "0.9936228046381672\n",
      "0.9945124252879886\n",
      "0.9953830774804577\n",
      "0.9956429453461839\n",
      "0.9957662403189735\n",
      "0.9970807544134858\n",
      "0.997617561756555\n",
      "0.9987727408093083\n",
      "0.999281095466349\n",
      "0.9995144074917819\n",
      "0.9996016623956023\n",
      "0.9996661551506\n",
      "0.9998691176442693\n",
      "0.9996775362250114\n",
      "0.9998804987186807\n",
      "0.9999241261705909\n",
      "0.9998596334155933\n",
      "0.9999601662395602\n",
      "0.9999791346969125\n",
      "0.9999184356333852\n",
      "0.9999886189255887\n",
      "0.9999279198620613\n",
      "0.9998729113357397\n",
      "0.999965856776766\n",
      "0.999975341005442\n",
      "0.9999696504682364\n",
      "0.9999905157713239\n",
      "0.9999924126170591\n",
      "0.9999032608675034\n",
      "0.9999886189255885\n",
      "0.9996851236079524\n",
      "0.9999962063085296\n",
      "0.999941197782208\n",
      "0.9999639599310306\n",
      "0.9999924126170591\n",
      "0.9998975703302978\n",
      "0.9999943094627943\n",
      "0.999975341005442\n",
      "0.9999905157713239\n",
      "0.9999336103992671\n",
      "0.9999487851651488\n",
      "0.9999487851651488\n",
      "0.9999905157713238\n",
      "0.9999924126170591\n",
      "0.9999867220798534\n",
      "0.999982928388383\n",
      "0.9999981031542647\n",
      "0.9999468883194137\n"
     ]
    }
   ],
   "source": [
    "for j in range(max_epochs):\n",
    "    #atnm,train,opti,crit,eval_metrics,iterator\n",
    "    n_iter=(train_ds.__len__()/batch_size)*j\n",
    "    roc_t=train_eval(sf,True,optimizer,criterion,True\n",
    "                   ,iter(train_dl),n_iter=n_iter,writer=writer)\n",
    "    roc_v=train_eval(atnm=sf\n",
    "                     ,train=False\n",
    "                     ,opti=optimizer\n",
    "                     ,crit=criterion\n",
    "                     ,eval_metrics=True\n",
    "                     ,iterator=iter(val_dl)\n",
    "                    ,writer=writer\n",
    "                    ,n_iter=n_iter\n",
    "                   )\n",
    "    \n",
    "    print(roc_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it=iter(val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining predictions\n",
    "\n",
    "In this loop we show that the model correctly recovers the \"reason\" for a fraud prediction as being the existence of one of the touple of problematic input elements. Here Self Attention generally performs much better. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 18]\n",
      " [ 9 18]\n",
      " [ 9 18]]\n",
      "[[ 9 18]\n",
      " [ 9 18]\n",
      " [ 1 17]\n",
      " [ 9 18]]\n",
      "predicted wrong\n"
     ]
    }
   ],
   "source": [
    "for batch in it:\n",
    "    #We get both inputs\n",
    "    i1=batch[0].long()\n",
    "    i2=batch[1].long()\n",
    "    \n",
    "    #We get both prediction and attention weights \n",
    "    predictions,weights=sf(i1,i2,batch[2].float(),True)\n",
    "    #we put the predictions and weights to numpy, \n",
    "    preds_np=np.round(predictions.detach().cpu().numpy())\n",
    "    weights_np=weights.detach().cpu().numpy()\n",
    "    #We only select the elments we predicted as a \n",
    "    fraud_elem=np.where(preds_np>0.5)[0]\n",
    "    #A prediction check, that we are correct\n",
    "    if fraud_elem.size>0:\n",
    "        if not(all(fraud_elem == np.where(batch[3].cpu().numpy())[0])) : \n",
    "            print(\"predicted wrong\")\n",
    "            break\n",
    "        #if len(np.where(fraud_elem))>0:\n",
    "        i1=i1.detach().cpu().numpy()\n",
    "        i2=i2.detach().cpu().numpy()   \n",
    "\n",
    "        for fraud_ele in fraud_elem:\n",
    "            my_weight=np.squeeze(np.round(weights_np[fraud_ele],2))\n",
    "            in_elem=np.stack([np.squeeze(i2[fraud_ele]),np.squeeze(i1[fraud_ele])],axis=1)\n",
    "\n",
    "            atn_elem=np.where(my_weight>0.5)[0]\n",
    "            in_elem_j=in_elem[atn_elem]\n",
    "\n",
    "            print(in_elem_j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
